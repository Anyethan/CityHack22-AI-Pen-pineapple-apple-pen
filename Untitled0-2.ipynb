{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9jnigQj94eUn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import time\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v83L4tT4mEs",
        "outputId": "b9c27a17-4400-4024-d730-8a0913f12dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yghLzzg447Vz"
      },
      "outputs": [],
      "source": [
        "train_csv = pd.read_csv(\"/content/drive/MyDrive/CityUHack22 AI/CH22_Demand_XY_Train.csv\")\n",
        "test_csv = pd.read_csv(\"/content/drive/MyDrive/CityUHack22 AI/CH22_Demand_raw_X_Test.csv\")\n",
        "#[\"DateTime\"]#.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEthUbqG_1Ti"
      },
      "source": [
        "DateTime Handle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r2YAgbWR6F83",
        "outputId": "5b51b43a-1321-47a2-c021-f86f50a173ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 DateTime        X1     X2      X3  ...  hour  min  day  date\n",
            "0     2022-01-01 00:00:00  2.186333  13.76  0.0663  ...     0    0    5     1\n",
            "1     2022-01-01 00:10:00  2.138000  13.90  0.0910  ...     0   10    5     1\n",
            "2     2022-01-01 00:20:00  2.104333  13.90  0.0806  ...     0   20    5     1\n",
            "3     2022-01-01 00:30:00  2.040333  14.00  0.1183  ...     0   30    5     1\n",
            "4     2022-01-01 00:40:00  1.973667  14.14  0.0624  ...     0   40    5     1\n",
            "...                   ...       ...    ...     ...  ...   ...  ...  ...   ...\n",
            "41927 2022-10-19 03:50:00  5.856667  17.66  0.1092  ...     3   50    2    19\n",
            "41928 2022-10-19 04:00:00  5.860000  17.66  0.1183  ...     4    0    2    19\n",
            "41929 2022-10-19 04:10:00  5.846667  17.68  0.1001  ...     4   10    2    19\n",
            "41930 2022-10-19 04:20:00  5.856667  17.66  0.1183  ...     4   20    2    19\n",
            "41931 2022-10-19 04:30:00  5.876667  17.68  0.0767  ...     4   30    2    19\n",
            "\n",
            "[41932 rows x 10 columns]\n",
            "                 DateTime        X1     X2  ...  day_cos  date_sin  date_cos\n",
            "0     2022-01-01 00:00:00  2.186333  13.76  ...      5.0  8.660254       5.0\n",
            "1     2022-01-01 00:10:00  2.138000  13.90  ...      5.0  8.660254       5.0\n",
            "2     2022-01-01 00:20:00  2.104333  13.90  ...      5.0  8.660254       5.0\n",
            "3     2022-01-01 00:30:00  2.040333  14.00  ...      5.0  8.660254       5.0\n",
            "4     2022-01-01 00:40:00  1.973667  14.14  ...      5.0  8.660254       5.0\n",
            "...                   ...       ...    ...  ...      ...       ...       ...\n",
            "41927 2022-10-19 03:50:00  5.856667  17.66  ...     -5.0  8.660254       5.0\n",
            "41928 2022-10-19 04:00:00  5.860000  17.66  ...     -5.0  8.660254       5.0\n",
            "41929 2022-10-19 04:10:00  5.846667  17.68  ...     -5.0  8.660254       5.0\n",
            "41930 2022-10-19 04:20:00  5.856667  17.66  ...     -5.0  8.660254       5.0\n",
            "41931 2022-10-19 04:30:00  5.876667  17.68  ...     -5.0  8.660254       5.0\n",
            "\n",
            "[41932 rows x 14 columns]\n",
            "                 DateTime        X1     X2      X3  ...  hour  min  day  date\n",
            "0     2022-10-19 04:40:00  5.943333  17.72  0.1001  ...     4   40    2    19\n",
            "1     2022-10-19 04:50:00  5.990000  17.68  0.1092  ...     4   50    2    19\n",
            "2     2022-10-19 05:00:00  5.993333  17.66  0.1235  ...     5    0    2    19\n",
            "3     2022-10-19 05:10:00  6.000000  17.68  0.0949  ...     5   10    2    19\n",
            "4     2022-10-19 05:20:00  6.013333  17.68  0.1326  ...     5   20    2    19\n",
            "...                   ...       ...    ...     ...  ...   ...  ...  ...   ...\n",
            "10479 2022-12-30 23:10:00  2.336667  13.48  0.0520  ...    23   10    4    30\n",
            "10480 2022-12-30 23:20:00  2.315667  13.52  0.0663  ...    23   20    4    30\n",
            "10481 2022-12-30 23:30:00  2.300000  13.56  0.1092  ...    23   30    4    30\n",
            "10482 2022-12-30 23:40:00  2.252667  13.60  0.0858  ...    23   40    4    30\n",
            "10483 2022-12-30 23:50:00  2.193333  13.82  0.0806  ...    23   50    4    30\n",
            "\n",
            "[10484 rows x 9 columns]\n",
            "                 DateTime        X1     X2  ...  day_cos      date_sin  date_cos\n",
            "0     2022-10-19 04:40:00  5.943333  17.72  ...     -5.0  8.660254e+00       5.0\n",
            "1     2022-10-19 04:50:00  5.990000  17.68  ...     -5.0  8.660254e+00       5.0\n",
            "2     2022-10-19 05:00:00  5.993333  17.66  ...     -5.0  8.660254e+00       5.0\n",
            "3     2022-10-19 05:10:00  6.000000  17.68  ...     -5.0  8.660254e+00       5.0\n",
            "4     2022-10-19 05:20:00  6.013333  17.68  ...     -5.0  8.660254e+00       5.0\n",
            "...                   ...       ...    ...  ...      ...           ...       ...\n",
            "10479 2022-12-30 23:10:00  2.336667  13.48  ...     -5.0 -4.777360e-14      10.0\n",
            "10480 2022-12-30 23:20:00  2.315667  13.52  ...     -5.0 -4.777360e-14      10.0\n",
            "10481 2022-12-30 23:30:00  2.300000  13.56  ...     -5.0 -4.777360e-14      10.0\n",
            "10482 2022-12-30 23:40:00  2.252667  13.60  ...     -5.0 -4.777360e-14      10.0\n",
            "10483 2022-12-30 23:50:00  2.193333  13.82  ...     -5.0 -4.777360e-14      10.0\n",
            "\n",
            "[10484 rows x 13 columns]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d429b065-44d5-48d3-bae7-8bce61d9c106\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DateTime</th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>Y</th>\n",
              "      <th>hour_sin</th>\n",
              "      <th>hour_cos</th>\n",
              "      <th>min_sin</th>\n",
              "      <th>min_cos</th>\n",
              "      <th>day_sin</th>\n",
              "      <th>day_cos</th>\n",
              "      <th>date_sin</th>\n",
              "      <th>date_cos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-01-01 00:00:00</td>\n",
              "      <td>2.186333</td>\n",
              "      <td>13.76</td>\n",
              "      <td>0.0663</td>\n",
              "      <td>0.1547</td>\n",
              "      <td>521163.83540</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>-8.660254</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-01-01 00:10:00</td>\n",
              "      <td>2.138000</td>\n",
              "      <td>13.90</td>\n",
              "      <td>0.0910</td>\n",
              "      <td>0.1105</td>\n",
              "      <td>449066.62018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.510565e+00</td>\n",
              "      <td>3.09017</td>\n",
              "      <td>-8.660254</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-01-01 00:20:00</td>\n",
              "      <td>2.104333</td>\n",
              "      <td>13.90</td>\n",
              "      <td>0.0806</td>\n",
              "      <td>0.1300</td>\n",
              "      <td>437394.72159</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.877853e+00</td>\n",
              "      <td>-8.09017</td>\n",
              "      <td>-8.660254</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-01-01 00:30:00</td>\n",
              "      <td>2.040333</td>\n",
              "      <td>14.00</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.1248</td>\n",
              "      <td>422107.63292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>-5.877853e+00</td>\n",
              "      <td>-8.09017</td>\n",
              "      <td>-8.660254</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-01-01 00:40:00</td>\n",
              "      <td>1.973667</td>\n",
              "      <td>14.14</td>\n",
              "      <td>0.0624</td>\n",
              "      <td>0.1105</td>\n",
              "      <td>406923.83540</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>-9.510565e+00</td>\n",
              "      <td>3.09017</td>\n",
              "      <td>-8.660254</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41927</th>\n",
              "      <td>2022-10-19 03:50:00</td>\n",
              "      <td>5.856667</td>\n",
              "      <td>17.66</td>\n",
              "      <td>0.1092</td>\n",
              "      <td>0.1391</td>\n",
              "      <td>365929.91028</td>\n",
              "      <td>7.308360</td>\n",
              "      <td>6.825531</td>\n",
              "      <td>-2.449294e-15</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41928</th>\n",
              "      <td>2022-10-19 04:00:00</td>\n",
              "      <td>5.860000</td>\n",
              "      <td>17.66</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.1495</td>\n",
              "      <td>368822.51417</td>\n",
              "      <td>8.878852</td>\n",
              "      <td>4.600650</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41929</th>\n",
              "      <td>2022-10-19 04:10:00</td>\n",
              "      <td>5.846667</td>\n",
              "      <td>17.68</td>\n",
              "      <td>0.1001</td>\n",
              "      <td>0.1976</td>\n",
              "      <td>373857.78769</td>\n",
              "      <td>8.878852</td>\n",
              "      <td>4.600650</td>\n",
              "      <td>9.510565e+00</td>\n",
              "      <td>3.09017</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41930</th>\n",
              "      <td>2022-10-19 04:20:00</td>\n",
              "      <td>5.856667</td>\n",
              "      <td>17.66</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.1391</td>\n",
              "      <td>373536.38739</td>\n",
              "      <td>8.878852</td>\n",
              "      <td>4.600650</td>\n",
              "      <td>5.877853e+00</td>\n",
              "      <td>-8.09017</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41931</th>\n",
              "      <td>2022-10-19 04:30:00</td>\n",
              "      <td>5.876667</td>\n",
              "      <td>17.68</td>\n",
              "      <td>0.0767</td>\n",
              "      <td>0.1690</td>\n",
              "      <td>376643.25826</td>\n",
              "      <td>8.878852</td>\n",
              "      <td>4.600650</td>\n",
              "      <td>-5.877853e+00</td>\n",
              "      <td>-8.09017</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>8.660254</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41932 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d429b065-44d5-48d3-bae7-8bce61d9c106')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d429b065-44d5-48d3-bae7-8bce61d9c106 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d429b065-44d5-48d3-bae7-8bce61d9c106');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 DateTime        X1     X2  ...  day_cos  date_sin  date_cos\n",
              "0     2022-01-01 00:00:00  2.186333  13.76  ...      5.0  8.660254       5.0\n",
              "1     2022-01-01 00:10:00  2.138000  13.90  ...      5.0  8.660254       5.0\n",
              "2     2022-01-01 00:20:00  2.104333  13.90  ...      5.0  8.660254       5.0\n",
              "3     2022-01-01 00:30:00  2.040333  14.00  ...      5.0  8.660254       5.0\n",
              "4     2022-01-01 00:40:00  1.973667  14.14  ...      5.0  8.660254       5.0\n",
              "...                   ...       ...    ...  ...      ...       ...       ...\n",
              "41927 2022-10-19 03:50:00  5.856667  17.66  ...     -5.0  8.660254       5.0\n",
              "41928 2022-10-19 04:00:00  5.860000  17.66  ...     -5.0  8.660254       5.0\n",
              "41929 2022-10-19 04:10:00  5.846667  17.68  ...     -5.0  8.660254       5.0\n",
              "41930 2022-10-19 04:20:00  5.856667  17.66  ...     -5.0  8.660254       5.0\n",
              "41931 2022-10-19 04:30:00  5.876667  17.68  ...     -5.0  8.660254       5.0\n",
              "\n",
              "[41932 rows x 14 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def feature(demand_tr):\n",
        "  def encode(data, col, max_val):\n",
        "      data[col+'_sin'] = np.sin(2 * np.pi * data[col]/max_val)*10\n",
        "      data[col+'_cos'] = np.cos(2 * np.pi * data[col]/max_val)*10\n",
        "      return data\n",
        "  demand_tr[\"DateTime\"] =  demand_tr[\"DateTime\"].apply(lambda x: x[2:])\n",
        "  demand_tr[\"DateTime\"] = demand_tr[\"DateTime\"].apply(lambda x :datetime.strptime(x, '%y-%m-%d %H:%M:%S'))\n",
        "\n",
        "\n",
        "  demand_tr['hour'] = demand_tr['DateTime'].dt.hour\n",
        "  demand_tr['min'] = demand_tr['DateTime'].dt.minute\n",
        "  demand_tr['day'] = demand_tr['DateTime'].dt.weekday\n",
        "  demand_tr['date'] = demand_tr['DateTime'].dt.day\n",
        "  #demand_tr['month'] = demand_tr['DateTime'].dt.month.astype(int)\n",
        "\n",
        "  print(demand_tr)\n",
        "  encode(demand_tr, 'hour', demand_tr['hour'].max() )\n",
        "  encode(demand_tr, 'min', demand_tr['min'].max() )\n",
        "  encode(demand_tr, 'day', demand_tr['day'].max() )\n",
        "  encode(demand_tr, 'date', demand_tr['day'].max() )\n",
        "\n",
        "  #demand_tr[\"X1\"] = demand_tr[\"X1\"]*10\n",
        "\n",
        "  demand_tr = demand_tr.drop(['hour','min','day','date'], axis =1)\n",
        "\n",
        "  print(demand_tr)\n",
        "  return demand_tr\n",
        "\n",
        "tr_f = feature(train_csv)\n",
        "te_f = feature(test_csv)\n",
        "tr_f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z32M9f6wBgzv"
      },
      "source": [
        "Train Validation Set Divison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rOd8U6IvlfH",
        "outputId": "16b2767c-4f26-426d-d2d3-fa5c5d7ac8ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             X1     X2      X3  ...  day_cos      date_sin  date_cos\n",
            "0      5.943333  17.72  0.1001  ...     -5.0  8.660254e+00       5.0\n",
            "1      5.990000  17.68  0.1092  ...     -5.0  8.660254e+00       5.0\n",
            "2      5.993333  17.66  0.1235  ...     -5.0  8.660254e+00       5.0\n",
            "3      6.000000  17.68  0.0949  ...     -5.0  8.660254e+00       5.0\n",
            "4      6.013333  17.68  0.1326  ...     -5.0  8.660254e+00       5.0\n",
            "...         ...    ...     ...  ...      ...           ...       ...\n",
            "10479  2.336667  13.48  0.0520  ...     -5.0 -4.777360e-14      10.0\n",
            "10480  2.315667  13.52  0.0663  ...     -5.0 -4.777360e-14      10.0\n",
            "10481  2.300000  13.56  0.1092  ...     -5.0 -4.777360e-14      10.0\n",
            "10482  2.252667  13.60  0.0858  ...     -5.0 -4.777360e-14      10.0\n",
            "10483  2.193333  13.82  0.0806  ...     -5.0 -4.777360e-14      10.0\n",
            "\n",
            "[10484 rows x 12 columns]\n",
            "             X1        X2        X3  ...   day_cos  date_sin  date_cos\n",
            "0      0.499447  1.756925 -0.632771  ... -0.818989  1.207968  0.701247\n",
            "1      0.534062  1.741566 -0.632730  ... -0.818989  1.207968  0.701247\n",
            "2      0.536535  1.733886 -0.632666  ... -0.818989  1.207968  0.701247\n",
            "3      0.541480  1.741566 -0.632794  ... -0.818989  1.207968  0.701247\n",
            "4      0.551370  1.741566 -0.632626  ... -0.818989  1.207968  0.701247\n",
            "...         ...       ...       ...  ...       ...       ...       ...\n",
            "10479 -2.175815  0.128817 -0.632986  ... -0.818989 -0.013515  1.410340\n",
            "10480 -2.191392  0.144177 -0.632922  ... -0.818989 -0.013515  1.410340\n",
            "10481 -2.203013  0.159536 -0.632730  ... -0.818989 -0.013515  1.410340\n",
            "10482 -2.238123  0.174896 -0.632835  ... -0.818989 -0.013515  1.410340\n",
            "10483 -2.282134  0.259373 -0.632858  ... -0.818989 -0.013515  1.410340\n",
            "\n",
            "[10484 rows x 12 columns]\n",
            "torch.Size([10484, 12])\n"
          ]
        }
      ],
      "source": [
        "demand_tr = tr_f\n",
        "\n",
        "tr = demand_tr[:int(len(demand_tr)*0.85)]\n",
        "vali = demand_tr[int(len(demand_tr)*0.85):]\n",
        "tr_feat = tr.drop([\"DateTime\", \"Y\"], axis=1)\n",
        "tr_label = tr[\"Y\"]\n",
        "vali_feat = vali.drop([\"DateTime\", \"Y\"], axis=1)\n",
        "vali_label = vali[\"Y\"]\n",
        "\n",
        "tr_feat -= np.mean(tr_feat, axis = 0)\n",
        "tr_feat /= np.std(tr_feat, axis = 0)\n",
        "vali_feat -= np.mean(vali_feat, axis = 0)\n",
        "vali_feat /= np.std(vali_feat, axis = 0)\n",
        "\n",
        "\n",
        "tr_input = torch.tensor(tr_feat.to_numpy())\n",
        "tr_lab = torch.tensor(tr_label.to_numpy())\n",
        "vali_input = torch.tensor(vali_feat.to_numpy())\n",
        "vali_lab = torch.tensor(vali_label.to_numpy())\n",
        "b_size = 512\n",
        "\n",
        "te_feat = te_f.drop([\"DateTime\"], axis=1)\n",
        "print(te_feat)\n",
        "te_feat -= np.mean(te_feat, axis = 0)\n",
        "te_feat /= np.std(te_feat, axis = 0)\n",
        "print(te_feat)\n",
        "\n",
        "te_input = torch.tensor(te_feat.to_numpy())\n",
        "print(te_input.size())\n",
        "\n",
        "train_data = torch.utils.data.TensorDataset(tr_input,tr_lab)#(input_vol_t[:80000],labels_t[:80000]) \n",
        "vali_data = torch.utils.data.TensorDataset(vali_input,vali_lab)\n",
        "test_data = torch.utils.data.TensorDataset(te_input)\n",
        "\n",
        "#print(train_data[0])\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = b_size)  \n",
        "vali_loader = torch.utils.data.DataLoader(vali_data, batch_size = b_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size = b_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ek2SSgsbyMxq"
      },
      "outputs": [],
      "source": [
        "class myLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(myLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(12,256,2,batch_first=True, bidirectional = True)  #double the hidden size, better\n",
        "        #self.bn = nn.BatchNorm1d(1)\n",
        "        self.fc1 = nn.Linear(512,128)  #for bi-directional, will get double of the hidden\n",
        "\n",
        "        self.dp = nn.Dropout(p=0.2)\n",
        "        self.fc2 = nn.Linear(128,1)\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x,states = self.lstm(x) #LSTM returns tuple including tensor and others\n",
        "        #print(x, x.size())\n",
        "        x = self.dp(x)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.dp(x)\n",
        "        x = nn.functional.relu(self.fc2(x))#for large size 400, one more layer, better to handle the info\n",
        "     \n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzaDkZlttSww"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rff8yennzVgu"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = myLSTM()\n",
        "epoch =100\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9)\n",
        "MAE = torch.nn.L1Loss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOJCDyiUzl6-",
        "outputId": "e6877560-8cca-41ec-ffe4-0bee83ddf13c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 100 ========\n",
            "Training...\n",
            "[0.009000000000000001]\n",
            "train time for epoch 0 :  0.6716325283050537\n",
            "train loss for epoch 0 :  495737.87053571426\n",
            "Validating...\n",
            "vali loss for epoch 0 :  494389.02403846156\n",
            "\n",
            "======== Epoch 2 / 100 ========\n",
            "Training...\n",
            "[0.008100000000000001]\n",
            "train time for epoch 1 :  0.4592478275299072\n",
            "train loss for epoch 1 :  445286.07544642856\n",
            "Validating...\n",
            "vali loss for epoch 1 :  414137.94471153844\n",
            "\n",
            "======== Epoch 3 / 100 ========\n",
            "Training...\n",
            "[0.007290000000000001]\n",
            "train time for epoch 2 :  0.4530513286590576\n",
            "train loss for epoch 2 :  342585.8611607143\n",
            "Validating...\n",
            "vali loss for epoch 2 :  291053.48076923075\n",
            "\n",
            "======== Epoch 4 / 100 ========\n",
            "Training...\n",
            "[0.006561000000000002]\n",
            "train time for epoch 3 :  0.46387815475463867\n",
            "train loss for epoch 3 :  208221.75\n",
            "Validating...\n",
            "vali loss for epoch 3 :  148621.50120192306\n",
            "\n",
            "======== Epoch 5 / 100 ========\n",
            "Training...\n",
            "[0.005904900000000002]\n",
            "train time for epoch 4 :  0.4636235237121582\n",
            "train loss for epoch 4 :  124349.47946428572\n",
            "Validating...\n",
            "vali loss for epoch 4 :  101474.97175480769\n",
            "\n",
            "======== Epoch 6 / 100 ========\n",
            "Training...\n",
            "[0.005314410000000002]\n",
            "train time for epoch 5 :  0.5299460887908936\n",
            "train loss for epoch 5 :  107439.34430803571\n",
            "Validating...\n",
            "vali loss for epoch 5 :  93178.37620192308\n",
            "\n",
            "======== Epoch 7 / 100 ========\n",
            "Training...\n",
            "[0.004782969000000002]\n",
            "train time for epoch 6 :  0.4459681510925293\n",
            "train loss for epoch 6 :  106314.01149553571\n",
            "Validating...\n",
            "vali loss for epoch 6 :  92089.5390625\n",
            "\n",
            "======== Epoch 8 / 100 ========\n",
            "Training...\n",
            "[0.004304672100000002]\n",
            "train time for epoch 7 :  0.46521592140197754\n",
            "train loss for epoch 7 :  106253.58225446429\n",
            "Validating...\n",
            "vali loss for epoch 7 :  91938.18870192308\n",
            "\n",
            "======== Epoch 9 / 100 ========\n",
            "Training...\n",
            "[0.003874204890000002]\n",
            "train time for epoch 8 :  0.49931764602661133\n",
            "train loss for epoch 8 :  106371.48325892857\n",
            "Validating...\n",
            "vali loss for epoch 8 :  91923.06490384616\n",
            "\n",
            "======== Epoch 10 / 100 ========\n",
            "Training...\n",
            "[0.003486784401000002]\n",
            "train time for epoch 9 :  0.462878942489624\n",
            "train loss for epoch 9 :  106225.65714285715\n",
            "Validating...\n",
            "vali loss for epoch 9 :  91919.94471153847\n",
            "\n",
            "======== Epoch 11 / 100 ========\n",
            "Training...\n",
            "[0.003138105960900002]\n",
            "train time for epoch 10 :  0.45343542098999023\n",
            "train loss for epoch 10 :  106108.08236607142\n",
            "Validating...\n",
            "vali loss for epoch 10 :  91398.84375\n",
            "\n",
            "======== Epoch 12 / 100 ========\n",
            "Training...\n",
            "[0.0028242953648100018]\n",
            "train time for epoch 11 :  0.5219957828521729\n",
            "train loss for epoch 11 :  89971.40150669643\n",
            "Validating...\n",
            "vali loss for epoch 11 :  65653.04897836539\n",
            "\n",
            "======== Epoch 13 / 100 ========\n",
            "Training...\n",
            "[0.0025418658283290017]\n",
            "train time for epoch 12 :  0.45043134689331055\n",
            "train loss for epoch 12 :  71014.42243303571\n",
            "Validating...\n",
            "vali loss for epoch 12 :  55056.84194711538\n",
            "\n",
            "======== Epoch 14 / 100 ========\n",
            "Training...\n",
            "[0.0022876792454961017]\n",
            "train time for epoch 13 :  0.4513249397277832\n",
            "train loss for epoch 13 :  67808.80396205357\n",
            "Validating...\n",
            "vali loss for epoch 13 :  52090.02554086538\n",
            "\n",
            "======== Epoch 15 / 100 ========\n",
            "Training...\n",
            "[0.0020589113209464917]\n",
            "train time for epoch 14 :  0.5030984878540039\n",
            "train loss for epoch 14 :  65461.82237723214\n",
            "Validating...\n",
            "vali loss for epoch 14 :  50066.848557692305\n",
            "\n",
            "======== Epoch 16 / 100 ========\n",
            "Training...\n",
            "[0.0018530201888518425]\n",
            "train time for epoch 15 :  0.45331501960754395\n",
            "train loss for epoch 15 :  63912.448214285716\n",
            "Validating...\n",
            "vali loss for epoch 15 :  50417.43569711538\n",
            "\n",
            "======== Epoch 17 / 100 ========\n",
            "Training...\n",
            "[0.0016677181699666583]\n",
            "train time for epoch 16 :  0.4565584659576416\n",
            "train loss for epoch 16 :  61748.104296875\n",
            "Validating...\n",
            "vali loss for epoch 16 :  51120.34555288462\n",
            "\n",
            "======== Epoch 18 / 100 ========\n",
            "Training...\n",
            "[0.0015009463529699924]\n",
            "train time for epoch 17 :  0.4547889232635498\n",
            "train loss for epoch 17 :  60334.431194196426\n",
            "Validating...\n",
            "vali loss for epoch 17 :  50738.937199519234\n",
            "\n",
            "======== Epoch 19 / 100 ========\n",
            "Training...\n",
            "[0.0013508517176729932]\n",
            "train time for epoch 18 :  0.45664477348327637\n",
            "train loss for epoch 18 :  59052.363002232145\n",
            "Validating...\n",
            "vali loss for epoch 18 :  49867.227463942305\n",
            "\n",
            "======== Epoch 20 / 100 ========\n",
            "Training...\n",
            "[0.001215766545905694]\n",
            "train time for epoch 19 :  0.4418208599090576\n",
            "train loss for epoch 19 :  57901.61741071429\n",
            "Validating...\n",
            "vali loss for epoch 19 :  49491.88671875\n",
            "\n",
            "======== Epoch 21 / 100 ========\n",
            "Training...\n",
            "[0.0010941898913151245]\n",
            "train time for epoch 20 :  0.4688889980316162\n",
            "train loss for epoch 20 :  56978.033482142855\n",
            "Validating...\n",
            "vali loss for epoch 20 :  48655.28185096154\n",
            "\n",
            "======== Epoch 22 / 100 ========\n",
            "Training...\n",
            "[0.0009847709021836122]\n",
            "train time for epoch 21 :  0.4658067226409912\n",
            "train loss for epoch 21 :  56004.7015625\n",
            "Validating...\n",
            "vali loss for epoch 21 :  48135.703425480766\n",
            "\n",
            "======== Epoch 23 / 100 ========\n",
            "Training...\n",
            "[0.0008862938119652509]\n",
            "train time for epoch 22 :  0.5128633975982666\n",
            "train loss for epoch 22 :  55208.90256696429\n",
            "Validating...\n",
            "vali loss for epoch 22 :  47859.46213942308\n",
            "\n",
            "======== Epoch 24 / 100 ========\n",
            "Training...\n",
            "[0.0007976644307687258]\n",
            "train time for epoch 23 :  0.8896303176879883\n",
            "train loss for epoch 23 :  54650.071261160716\n",
            "Validating...\n",
            "vali loss for epoch 23 :  47892.39242788462\n",
            "\n",
            "======== Epoch 25 / 100 ========\n",
            "Training...\n",
            "[0.0007178979876918532]\n",
            "train time for epoch 24 :  0.9362781047821045\n",
            "train loss for epoch 24 :  53946.309375\n",
            "Validating...\n",
            "vali loss for epoch 24 :  47737.3359375\n",
            "\n",
            "======== Epoch 26 / 100 ========\n",
            "Training...\n",
            "[0.0006461081889226679]\n",
            "train time for epoch 25 :  0.7119722366333008\n",
            "train loss for epoch 25 :  53531.53989955357\n",
            "Validating...\n",
            "vali loss for epoch 25 :  47606.26051682692\n",
            "\n",
            "======== Epoch 27 / 100 ========\n",
            "Training...\n",
            "[0.0005814973700304011]\n",
            "train time for epoch 26 :  0.50016188621521\n",
            "train loss for epoch 26 :  53327.95128348214\n",
            "Validating...\n",
            "vali loss for epoch 26 :  47844.03515625\n",
            "\n",
            "======== Epoch 28 / 100 ========\n",
            "Training...\n",
            "[0.0005233476330273611]\n",
            "train time for epoch 27 :  0.5014185905456543\n",
            "train loss for epoch 27 :  52708.84949776786\n",
            "Validating...\n",
            "vali loss for epoch 27 :  47306.627403846156\n",
            "\n",
            "======== Epoch 29 / 100 ========\n",
            "Training...\n",
            "[0.000471012869724625]\n",
            "train time for epoch 28 :  0.45769333839416504\n",
            "train loss for epoch 28 :  52203.38565848214\n",
            "Validating...\n",
            "vali loss for epoch 28 :  47585.377403846156\n",
            "\n",
            "======== Epoch 30 / 100 ========\n",
            "Training...\n",
            "[0.0004239115827521625]\n",
            "train time for epoch 29 :  0.5171401500701904\n",
            "train loss for epoch 29 :  51730.806361607145\n",
            "Validating...\n",
            "vali loss for epoch 29 :  47777.44711538462\n",
            "\n",
            "======== Epoch 31 / 100 ========\n",
            "Training...\n",
            "[0.00038152042447694626]\n",
            "train time for epoch 30 :  0.449688196182251\n",
            "train loss for epoch 30 :  51645.05256696429\n",
            "Validating...\n",
            "vali loss for epoch 30 :  47839.286057692305\n",
            "\n",
            "======== Epoch 32 / 100 ========\n",
            "Training...\n",
            "[0.00034336838202925164]\n",
            "train time for epoch 31 :  0.4534778594970703\n",
            "train loss for epoch 31 :  51113.032421875\n",
            "Validating...\n",
            "vali loss for epoch 31 :  47698.09825721154\n",
            "\n",
            "======== Epoch 33 / 100 ========\n",
            "Training...\n",
            "[0.0003090315438263265]\n",
            "train time for epoch 32 :  0.5096380710601807\n",
            "train loss for epoch 32 :  50923.57047991071\n",
            "Validating...\n",
            "vali loss for epoch 32 :  47744.62830528846\n",
            "\n",
            "======== Epoch 34 / 100 ========\n",
            "Training...\n",
            "[0.00027812838944369386]\n",
            "train time for epoch 33 :  0.45773959159851074\n",
            "train loss for epoch 33 :  50660.12181919643\n",
            "Validating...\n",
            "vali loss for epoch 33 :  47916.790564903844\n",
            "\n",
            "======== Epoch 35 / 100 ========\n",
            "Training...\n",
            "[0.0002503155504993245]\n",
            "train time for epoch 34 :  0.4591968059539795\n",
            "train loss for epoch 34 :  50366.34553571429\n",
            "Validating...\n",
            "vali loss for epoch 34 :  48057.027043269234\n",
            "\n",
            "======== Epoch 36 / 100 ========\n",
            "Training...\n",
            "[0.00022528399544939206]\n",
            "train time for epoch 35 :  0.5198843479156494\n",
            "train loss for epoch 35 :  50303.208035714284\n",
            "Validating...\n",
            "vali loss for epoch 35 :  48243.13010817308\n",
            "\n",
            "======== Epoch 37 / 100 ========\n",
            "Training...\n",
            "[0.00020275559590445286]\n",
            "train time for epoch 36 :  0.4497673511505127\n",
            "train loss for epoch 36 :  50177.75050223214\n",
            "Validating...\n",
            "vali loss for epoch 36 :  48391.597956730766\n",
            "\n",
            "======== Epoch 38 / 100 ========\n",
            "Training...\n",
            "[0.00018248003631400757]\n",
            "train time for epoch 37 :  0.4609241485595703\n",
            "train loss for epoch 37 :  50204.56422991071\n",
            "Validating...\n",
            "vali loss for epoch 37 :  48397.819411057695\n",
            "\n",
            "======== Epoch 39 / 100 ========\n",
            "Training...\n",
            "[0.00016423203268260683]\n",
            "train time for epoch 38 :  0.4578685760498047\n",
            "train loss for epoch 38 :  49558.061160714286\n",
            "Validating...\n",
            "vali loss for epoch 38 :  48539.883112980766\n",
            "\n",
            "======== Epoch 40 / 100 ========\n",
            "Training...\n",
            "[0.00014780882941434616]\n",
            "train time for epoch 39 :  0.4572882652282715\n",
            "train loss for epoch 39 :  49387.24291294643\n",
            "Validating...\n",
            "vali loss for epoch 39 :  48564.992487980766\n",
            "\n",
            "======== Epoch 41 / 100 ========\n",
            "Training...\n",
            "[0.00013302794647291155]\n",
            "train time for epoch 40 :  0.4521970748901367\n",
            "train loss for epoch 40 :  49670.06060267857\n",
            "Validating...\n",
            "vali loss for epoch 40 :  48464.25\n",
            "\n",
            "======== Epoch 42 / 100 ========\n",
            "Training...\n",
            "[0.00011972515182562039]\n",
            "train time for epoch 41 :  0.7252459526062012\n",
            "train loss for epoch 41 :  49482.6875\n",
            "Validating...\n",
            "vali loss for epoch 41 :  48485.898737980766\n",
            "\n",
            "======== Epoch 43 / 100 ========\n",
            "Training...\n",
            "[0.00010775263664305835]\n",
            "train time for epoch 42 :  1.8791160583496094\n",
            "train loss for epoch 42 :  49358.120145089284\n",
            "Validating...\n",
            "vali loss for epoch 42 :  48550.946213942305\n",
            "\n",
            "======== Epoch 44 / 100 ========\n",
            "Training...\n",
            "[9.697737297875251e-05]\n",
            "train time for epoch 43 :  1.5344185829162598\n",
            "train loss for epoch 43 :  49242.69402901785\n",
            "Validating...\n",
            "vali loss for epoch 43 :  48533.16736778846\n",
            "\n",
            "======== Epoch 45 / 100 ========\n",
            "Training...\n",
            "[8.727963568087727e-05]\n",
            "train time for epoch 44 :  0.9696755409240723\n",
            "train loss for epoch 44 :  49254.648158482145\n",
            "Validating...\n",
            "vali loss for epoch 44 :  48512.93539663462\n",
            "\n",
            "======== Epoch 46 / 100 ========\n",
            "Training...\n",
            "[7.855167211278955e-05]\n",
            "train time for epoch 45 :  0.9272172451019287\n",
            "train loss for epoch 45 :  49192.35267857143\n",
            "Validating...\n",
            "vali loss for epoch 45 :  48510.28725961538\n",
            "\n",
            "======== Epoch 47 / 100 ========\n",
            "Training...\n",
            "[7.06965049015106e-05]\n",
            "train time for epoch 46 :  1.0763072967529297\n",
            "train loss for epoch 46 :  49094.70329241071\n",
            "Validating...\n",
            "vali loss for epoch 46 :  48532.561598557695\n",
            "\n",
            "======== Epoch 48 / 100 ========\n",
            "Training...\n",
            "[6.362685441135955e-05]\n",
            "train time for epoch 47 :  1.0301141738891602\n",
            "train loss for epoch 47 :  49171.76635044643\n",
            "Validating...\n",
            "vali loss for epoch 47 :  48541.85606971154\n",
            "\n",
            "======== Epoch 49 / 100 ========\n",
            "Training...\n",
            "[5.7264168970223595e-05]\n",
            "train time for epoch 48 :  0.9746289253234863\n",
            "train loss for epoch 48 :  49325.08470982143\n",
            "Validating...\n",
            "vali loss for epoch 48 :  48591.65234375\n",
            "\n",
            "======== Epoch 50 / 100 ========\n",
            "Training...\n",
            "[5.153775207320124e-05]\n",
            "train time for epoch 49 :  0.9015905857086182\n",
            "train loss for epoch 49 :  48967.63035714286\n",
            "Validating...\n",
            "vali loss for epoch 49 :  48607.312800480766\n",
            "\n",
            "======== Epoch 51 / 100 ========\n",
            "Training...\n",
            "[4.6383976865881114e-05]\n",
            "train time for epoch 50 :  0.789649486541748\n",
            "train loss for epoch 50 :  48794.98258928571\n",
            "Validating...\n",
            "vali loss for epoch 50 :  48612.145432692305\n",
            "\n",
            "======== Epoch 52 / 100 ========\n",
            "Training...\n",
            "[4.1745579179293e-05]\n",
            "train time for epoch 51 :  1.3170578479766846\n",
            "train loss for epoch 51 :  48901.80401785715\n",
            "Validating...\n",
            "vali loss for epoch 51 :  48575.295973557695\n",
            "\n",
            "======== Epoch 53 / 100 ========\n",
            "Training...\n",
            "[3.75710212613637e-05]\n",
            "train time for epoch 52 :  0.6050784587860107\n",
            "train loss for epoch 52 :  48971.55714285714\n",
            "Validating...\n",
            "vali loss for epoch 52 :  48554.52283653846\n",
            "\n",
            "======== Epoch 54 / 100 ========\n",
            "Training...\n",
            "[3.381391913522733e-05]\n",
            "train time for epoch 53 :  0.6037890911102295\n",
            "train loss for epoch 53 :  49196.946484375\n",
            "Validating...\n",
            "vali loss for epoch 53 :  48550.40685096154\n",
            "\n",
            "======== Epoch 55 / 100 ========\n",
            "Training...\n",
            "[3.0432527221704597e-05]\n",
            "train time for epoch 54 :  0.5746245384216309\n",
            "train loss for epoch 54 :  48947.639676339284\n",
            "Validating...\n",
            "vali loss for epoch 54 :  48562.678786057695\n",
            "\n",
            "======== Epoch 56 / 100 ========\n",
            "Training...\n",
            "[2.7389274499534138e-05]\n",
            "train time for epoch 55 :  0.8817458152770996\n",
            "train loss for epoch 55 :  48946.91958705357\n",
            "Validating...\n",
            "vali loss for epoch 55 :  48577.30528846154\n",
            "\n",
            "======== Epoch 57 / 100 ========\n",
            "Training...\n",
            "[2.4650347049580723e-05]\n",
            "train time for epoch 56 :  0.5214035511016846\n",
            "train loss for epoch 56 :  48785.71534598214\n",
            "Validating...\n",
            "vali loss for epoch 56 :  48590.007512019234\n",
            "\n",
            "======== Epoch 58 / 100 ========\n",
            "Training...\n",
            "[2.218531234462265e-05]\n",
            "train time for epoch 57 :  0.8726117610931396\n",
            "train loss for epoch 57 :  48937.18733258929\n",
            "Validating...\n",
            "vali loss for epoch 57 :  48577.32301682692\n",
            "\n",
            "======== Epoch 59 / 100 ========\n",
            "Training...\n",
            "[1.9966781110160387e-05]\n",
            "train time for epoch 58 :  0.5341863632202148\n",
            "train loss for epoch 58 :  48919.692912946426\n",
            "Validating...\n",
            "vali loss for epoch 58 :  48584.62289663462\n",
            "\n",
            "======== Epoch 60 / 100 ========\n",
            "Training...\n",
            "[1.797010299914435e-05]\n",
            "train time for epoch 59 :  0.7401547431945801\n",
            "train loss for epoch 59 :  48724.07611607143\n",
            "Validating...\n",
            "vali loss for epoch 59 :  48594.91466346154\n",
            "\n",
            "======== Epoch 61 / 100 ========\n",
            "Training...\n",
            "[1.6173092699229914e-05]\n",
            "train time for epoch 60 :  0.6291108131408691\n",
            "train loss for epoch 60 :  48680.03130580357\n",
            "Validating...\n",
            "vali loss for epoch 60 :  48601.317908653844\n",
            "\n",
            "======== Epoch 62 / 100 ========\n",
            "Training...\n",
            "[1.4555783429306922e-05]\n",
            "train time for epoch 61 :  0.7377026081085205\n",
            "train loss for epoch 61 :  48873.95965401785\n",
            "Validating...\n",
            "vali loss for epoch 61 :  48601.077223557695\n",
            "\n",
            "======== Epoch 63 / 100 ========\n",
            "Training...\n",
            "[1.310020508637623e-05]\n",
            "train time for epoch 62 :  0.6931238174438477\n",
            "train loss for epoch 62 :  48842.519140625\n",
            "Validating...\n",
            "vali loss for epoch 62 :  48602.10727163462\n",
            "\n",
            "======== Epoch 64 / 100 ========\n",
            "Training...\n",
            "[1.1790184577738607e-05]\n",
            "train time for epoch 63 :  0.563117265701294\n",
            "train loss for epoch 63 :  48597.86785714285\n",
            "Validating...\n",
            "vali loss for epoch 63 :  48610.494591346156\n",
            "\n",
            "======== Epoch 65 / 100 ========\n",
            "Training...\n",
            "[1.0611166119964747e-05]\n",
            "train time for epoch 64 :  0.7090442180633545\n",
            "train loss for epoch 64 :  48637.32806919643\n",
            "Validating...\n",
            "vali loss for epoch 64 :  48609.74609375\n",
            "\n",
            "======== Epoch 66 / 100 ========\n",
            "Training...\n",
            "[9.550049507968273e-06]\n",
            "train time for epoch 65 :  0.6506528854370117\n",
            "train loss for epoch 65 :  48832.112890625\n",
            "Validating...\n",
            "vali loss for epoch 65 :  48610.347355769234\n",
            "\n",
            "======== Epoch 67 / 100 ========\n",
            "Training...\n",
            "[8.595044557171446e-06]\n",
            "train time for epoch 66 :  1.517404556274414\n",
            "train loss for epoch 66 :  48648.401395089284\n",
            "Validating...\n",
            "vali loss for epoch 66 :  48611.678786057695\n",
            "\n",
            "======== Epoch 68 / 100 ========\n",
            "Training...\n",
            "[7.735540101454301e-06]\n",
            "train time for epoch 67 :  1.4832432270050049\n",
            "train loss for epoch 67 :  48790.625279017855\n",
            "Validating...\n",
            "vali loss for epoch 67 :  48614.019831730766\n",
            "\n",
            "======== Epoch 69 / 100 ========\n",
            "Training...\n",
            "[6.9619860913088715e-06]\n",
            "train time for epoch 68 :  0.8637871742248535\n",
            "train loss for epoch 68 :  48771.454631696426\n",
            "Validating...\n",
            "vali loss for epoch 68 :  48613.807692307695\n",
            "\n",
            "======== Epoch 70 / 100 ========\n",
            "Training...\n",
            "[6.265787482177985e-06]\n",
            "train time for epoch 69 :  1.1585657596588135\n",
            "train loss for epoch 69 :  48640.890011160714\n",
            "Validating...\n",
            "vali loss for epoch 69 :  48616.36538461538\n",
            "\n",
            "======== Epoch 71 / 100 ========\n",
            "Training...\n",
            "[5.639208733960187e-06]\n",
            "train time for epoch 70 :  1.055131196975708\n",
            "train loss for epoch 70 :  48707.44977678572\n",
            "Validating...\n",
            "vali loss for epoch 70 :  48615.93960336538\n",
            "\n",
            "======== Epoch 72 / 100 ========\n",
            "Training...\n",
            "[5.075287860564168e-06]\n",
            "train time for epoch 71 :  0.931572675704956\n",
            "train loss for epoch 71 :  49038.56847098214\n",
            "Validating...\n",
            "vali loss for epoch 71 :  48619.54176682692\n",
            "\n",
            "======== Epoch 73 / 100 ========\n",
            "Training...\n",
            "[4.5677590745077515e-06]\n",
            "train time for epoch 72 :  0.630836009979248\n",
            "train loss for epoch 72 :  49104.05859375\n",
            "Validating...\n",
            "vali loss for epoch 72 :  48616.95132211538\n",
            "\n",
            "======== Epoch 74 / 100 ========\n",
            "Training...\n",
            "[4.110983167056976e-06]\n",
            "train time for epoch 73 :  0.5355312824249268\n",
            "train loss for epoch 73 :  48842.09893973215\n",
            "Validating...\n",
            "vali loss for epoch 73 :  48618.10036057692\n",
            "\n",
            "======== Epoch 75 / 100 ========\n",
            "Training...\n",
            "[3.6998848503512788e-06]\n",
            "train time for epoch 74 :  0.5674748420715332\n",
            "train loss for epoch 74 :  48862.57985491071\n",
            "Validating...\n",
            "vali loss for epoch 74 :  48619.346754807695\n",
            "\n",
            "======== Epoch 76 / 100 ========\n",
            "Training...\n",
            "[3.329896365316151e-06]\n",
            "train time for epoch 75 :  0.7458674907684326\n",
            "train loss for epoch 75 :  48741.62645089286\n",
            "Validating...\n",
            "vali loss for epoch 75 :  48622.48647836538\n",
            "\n",
            "======== Epoch 77 / 100 ========\n",
            "Training...\n",
            "[2.9969067287845362e-06]\n",
            "train time for epoch 76 :  0.5839173793792725\n",
            "train loss for epoch 76 :  48743.075390625\n",
            "Validating...\n",
            "vali loss for epoch 76 :  48625.06069711538\n",
            "\n",
            "======== Epoch 78 / 100 ========\n",
            "Training...\n",
            "[2.6972160559060827e-06]\n",
            "train time for epoch 77 :  0.5463695526123047\n",
            "train loss for epoch 77 :  48827.08962053571\n",
            "Validating...\n",
            "vali loss for epoch 77 :  48628.762620192305\n",
            "\n",
            "======== Epoch 79 / 100 ========\n",
            "Training...\n",
            "[2.4274944503154745e-06]\n",
            "train time for epoch 78 :  0.6554768085479736\n",
            "train loss for epoch 78 :  48458.880412946426\n",
            "Validating...\n",
            "vali loss for epoch 78 :  48628.192908653844\n",
            "\n",
            "======== Epoch 80 / 100 ========\n",
            "Training...\n",
            "[2.1847450052839273e-06]\n",
            "train time for epoch 79 :  0.9469254016876221\n",
            "train loss for epoch 79 :  48784.35859375\n",
            "Validating...\n",
            "vali loss for epoch 79 :  48629.14903846154\n",
            "\n",
            "======== Epoch 81 / 100 ========\n",
            "Training...\n",
            "[1.9662705047555346e-06]\n",
            "train time for epoch 80 :  0.685819149017334\n",
            "train loss for epoch 80 :  48691.84581473214\n",
            "Validating...\n",
            "vali loss for epoch 80 :  48627.54116586538\n",
            "\n",
            "======== Epoch 82 / 100 ========\n",
            "Training...\n",
            "[1.7696434542799813e-06]\n",
            "train time for epoch 81 :  0.7873518466949463\n",
            "train loss for epoch 81 :  48681.73348214286\n",
            "Validating...\n",
            "vali loss for epoch 81 :  48627.018028846156\n",
            "\n",
            "======== Epoch 83 / 100 ========\n",
            "Training...\n",
            "[1.5926791088519833e-06]\n",
            "train time for epoch 82 :  0.6745026111602783\n",
            "train loss for epoch 82 :  48546.24877232143\n",
            "Validating...\n",
            "vali loss for epoch 82 :  48626.23046875\n",
            "\n",
            "======== Epoch 84 / 100 ========\n",
            "Training...\n",
            "[1.433411197966785e-06]\n",
            "train time for epoch 83 :  0.5106656551361084\n",
            "train loss for epoch 83 :  48635.826953125\n",
            "Validating...\n",
            "vali loss for epoch 83 :  48626.028846153844\n",
            "\n",
            "======== Epoch 85 / 100 ========\n",
            "Training...\n",
            "[1.2900700781701065e-06]\n",
            "train time for epoch 84 :  0.4694538116455078\n",
            "train loss for epoch 84 :  48684.74146205357\n",
            "Validating...\n",
            "vali loss for epoch 84 :  48625.56640625\n",
            "\n",
            "======== Epoch 86 / 100 ========\n",
            "Training...\n",
            "[1.161063070353096e-06]\n",
            "train time for epoch 85 :  0.45011019706726074\n",
            "train loss for epoch 85 :  48791.44140625\n",
            "Validating...\n",
            "vali loss for epoch 85 :  48624.999699519234\n",
            "\n",
            "======== Epoch 87 / 100 ========\n",
            "Training...\n",
            "[1.0449567633177863e-06]\n",
            "train time for epoch 86 :  0.459714412689209\n",
            "train loss for epoch 86 :  48824.62689732143\n",
            "Validating...\n",
            "vali loss for epoch 86 :  48625.112379807695\n",
            "\n",
            "======== Epoch 88 / 100 ========\n",
            "Training...\n",
            "[9.404610869860078e-07]\n",
            "train time for epoch 87 :  0.4537191390991211\n",
            "train loss for epoch 87 :  48857.81333705357\n",
            "Validating...\n",
            "vali loss for epoch 87 :  48624.5546875\n",
            "\n",
            "======== Epoch 89 / 100 ========\n",
            "Training...\n",
            "[8.46414978287407e-07]\n",
            "train time for epoch 88 :  0.46654820442199707\n",
            "train loss for epoch 88 :  48762.97589285715\n",
            "Validating...\n",
            "vali loss for epoch 88 :  48624.572716346156\n",
            "\n",
            "======== Epoch 90 / 100 ========\n",
            "Training...\n",
            "[7.617734804586663e-07]\n",
            "train time for epoch 89 :  0.4519081115722656\n",
            "train loss for epoch 89 :  48829.75106026786\n",
            "Validating...\n",
            "vali loss for epoch 89 :  48624.466346153844\n",
            "\n",
            "======== Epoch 91 / 100 ========\n",
            "Training...\n",
            "[6.855961324127997e-07]\n",
            "train time for epoch 90 :  0.4553523063659668\n",
            "train loss for epoch 90 :  48788.69347098214\n",
            "Validating...\n",
            "vali loss for epoch 90 :  48624.33533653846\n",
            "\n",
            "======== Epoch 92 / 100 ========\n",
            "Training...\n",
            "[6.170365191715197e-07]\n",
            "train time for epoch 91 :  0.4996051788330078\n",
            "train loss for epoch 91 :  48671.07918526786\n",
            "Validating...\n",
            "vali loss for epoch 91 :  48623.689903846156\n",
            "\n",
            "======== Epoch 93 / 100 ========\n",
            "Training...\n",
            "[5.553328672543678e-07]\n",
            "train time for epoch 92 :  0.4625124931335449\n",
            "train loss for epoch 92 :  48598.10513392857\n",
            "Validating...\n",
            "vali loss for epoch 92 :  48623.47776442308\n",
            "\n",
            "======== Epoch 94 / 100 ========\n",
            "Training...\n",
            "[4.99799580528931e-07]\n",
            "train time for epoch 93 :  0.4563772678375244\n",
            "train loss for epoch 93 :  48797.407254464284\n",
            "Validating...\n",
            "vali loss for epoch 93 :  48623.27133413462\n",
            "\n",
            "======== Epoch 95 / 100 ========\n",
            "Training...\n",
            "[4.498196224760379e-07]\n",
            "train time for epoch 94 :  0.5244917869567871\n",
            "train loss for epoch 94 :  48710.53638392857\n",
            "Validating...\n",
            "vali loss for epoch 94 :  48623.153245192305\n",
            "\n",
            "======== Epoch 96 / 100 ========\n",
            "Training...\n",
            "[4.0483766022843414e-07]\n",
            "train time for epoch 95 :  0.4512186050415039\n",
            "train loss for epoch 95 :  48689.80323660714\n",
            "Validating...\n",
            "vali loss for epoch 95 :  48622.86448317308\n",
            "\n",
            "======== Epoch 97 / 100 ========\n",
            "Training...\n",
            "[3.643538942055907e-07]\n",
            "train time for epoch 96 :  0.5056476593017578\n",
            "train loss for epoch 96 :  48821.18348214286\n",
            "Validating...\n",
            "vali loss for epoch 96 :  48622.47265625\n",
            "\n",
            "======== Epoch 98 / 100 ========\n",
            "Training...\n",
            "[3.2791850478503163e-07]\n",
            "train time for epoch 97 :  0.44991254806518555\n",
            "train loss for epoch 97 :  48871.39821428571\n",
            "Validating...\n",
            "vali loss for epoch 97 :  48622.26442307692\n",
            "\n",
            "======== Epoch 99 / 100 ========\n",
            "Training...\n",
            "[2.951266543065285e-07]\n",
            "train time for epoch 98 :  0.44977831840515137\n",
            "train loss for epoch 98 :  48879.284709821426\n",
            "Validating...\n",
            "vali loss for epoch 98 :  48621.99489182692\n",
            "\n",
            "======== Epoch 100 / 100 ========\n",
            "Training...\n",
            "[2.6561398887587566e-07]\n",
            "train time for epoch 99 :  0.5037469863891602\n",
            "train loss for epoch 99 :  48696.39174107143\n",
            "Validating...\n",
            "vali loss for epoch 99 :  48621.972355769234\n"
          ]
        }
      ],
      "source": [
        "model.cuda()\n",
        "\n",
        "\n",
        "for each_epoch in range(epoch):\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(each_epoch + 1, epoch))\n",
        "    \n",
        "    print('Training...')\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    total_train_loss=0\n",
        "    \n",
        "    for step, batch in enumerate(train_loader):\n",
        "        device = 'cuda'\n",
        "        #b_input = batch[0].float().to(device).view(1, 2, len(batch[0]), 1,-1) #make it righr dimension 32,1,30\n",
        "        \n",
        "        \n",
        "        b_input = batch[0].float().to(device).view(len(batch[0]), 1,-1) #make it righr dimension 32,1,30\n",
        "        \n",
        "        b_label = batch[1].float().to(device).view(len(batch[0]), 1,-1) #also change for labels or it can broadcast\n",
        "        \n",
        "        \n",
        "        model.zero_grad()\n",
        "        pred = model(b_input) #return a tuple\n",
        "        \n",
        "        loss = MAE(pred, b_label)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "    scheduler.step()\n",
        "    print(scheduler.get_last_lr())\n",
        "    training_time = time.time() - t0 \n",
        "    avg_loss = total_train_loss / len(train_loader)\n",
        "    print(\"train time for epoch\",each_epoch,\": \",training_time)\n",
        "    print(\"train loss for epoch\" , each_epoch, \": \", avg_loss)\n",
        "    \n",
        "    print('Validating...')\n",
        "    model.eval()\n",
        "    \n",
        "    total_eval_loss=0\n",
        "\n",
        "    for step, batch in enumerate(vali_loader):\n",
        "        device = 'cuda:0'\n",
        "        b_input = batch[0].float().to(device).view(len(batch[0]), 1,-1) #32,1,30\n",
        "        b_label = batch[1].float().to(device).view(len(batch[0]), 1,-1)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            pred = model(b_input)\n",
        "            loss = MAE(pred, b_label)\n",
        "\n",
        "        \n",
        "        \n",
        "        total_eval_loss += loss.item()\n",
        "        \n",
        "    vali_loss = total_eval_loss / len(vali_loader)\n",
        "    print(\"vali loss for epoch\" , each_epoch, \": \", vali_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaE2Lf7dTE3n",
        "outputId": "fa0de944-1402-4923-9e14-654460e7d7a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                0\n",
            "0    402362.06250\n",
            "1    412545.31250\n",
            "2    362747.31250\n",
            "3    373369.43750\n",
            "4    367371.81250\n",
            "..            ...\n",
            "239  414456.87500\n",
            "240  411016.68750\n",
            "241  384325.31250\n",
            "242  374044.28125\n",
            "243  380922.81250\n",
            "\n",
            "[10484 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "device= \"cuda:0\"\n",
        "preds=pd.DataFrame()\n",
        "answer=pd.DataFrame()\n",
        "for step,batch in enumerate(test_loader):\n",
        "    b_input = batch[0].float().to(device).view(len(batch[0]), 1,-1) #make it righr dimension 32,1,19\n",
        "    with torch.no_grad():\n",
        "        prediction = model(b_input)\n",
        "    prediction = prediction.detach().cpu()\n",
        "\n",
        "    preds = preds.append(pd.DataFrame(prediction.view(len(b_input),1).numpy()))\n",
        "print(preds)\n",
        "\n",
        "test_csv[\"Y\"] = preds.values\n",
        "test_csv[[\"X1\",\"X2\",\"X3\",\"X4\",\"Y\"]].to_csv(\"test_result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "hsqFdTPxAoqQ",
        "outputId": "d62206c1-78f4-48db-9412-6bd11ea16d89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1585: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
            "  \"GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\"\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-e7a99d57df0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m nbeat =  pytorch_forecasting.models.nbeats.NBeats.from_dataset( training,loss=torch.nn.L1Loss(),\n\u001b[0;32m---> 16\u001b[0;31m                                                                \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                                                                )\n\u001b[1;32m     18\u001b[0m \u001b[0mnbeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_forecasting/models/nbeats/__init__.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_varying_unknown_reals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_varying_unknown_reals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         ), \"The only variable as input should be the target which is part of time_varying_unknown_reals\"\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# initialize class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: The only variable as input should be the target which is part of time_varying_unknown_reals"
          ]
        }
      ],
      "source": [
        "\"\"\"import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\n",
        "lr_logger = LearningRateMonitor()\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=100,\n",
        "    gpus=0,\n",
        "    gradient_clip_val=0.1,\n",
        "    limit_train_batches=30,\n",
        "    callbacks=[lr_logger, early_stop_callback],\n",
        ")\n",
        "\n",
        "nbeat =  pytorch_forecasting.models.nbeats.NBeats.from_dataset( training,loss=torch.nn.L1Loss(),\n",
        "                                                               learning_rate =1e-2,\n",
        "                                                               )\n",
        "nbeat.cuda()\n",
        "res = trainer.lr_find(\n",
        "    nbeat, train_dataloader=tr_nb_loader, val_dataloaders=va_nb_loader, early_stop_threshold=1000.0, max_lr=0.3,\n",
        ")\n",
        "fig = res.plot(show=True, suggest=True)\n",
        "fig.show()\n",
        "\n",
        "trainer.fit(\n",
        "    nbeat, train_dataloader=tr_nb_loader, val_dataloaders=va_nb_loader,\n",
        ")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4H2HSAeJkIP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK0YNMPqBdkI"
      },
      "outputs": [],
      "source": [
        "\"\"\"demand_tr = demand_tr[:10000]\n",
        "tr = demand_tr[:int(len(demand_tr)*0.85)]\n",
        "vali = demand_tr[int(len(demand_tr)*0.85):]\n",
        "\n",
        "x_values = tr.drop([\"DateTime\", \"Y\"], axis=1)\n",
        "\n",
        "y_values = tr[\"Y\"]\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeV-2M19EI_Y"
      },
      "outputs": [],
      "source": [
        "\"\"\"import torch\n",
        "\n",
        "class linearRegression(torch.nn.Module):\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubBjMOU9Mizt"
      },
      "outputs": [],
      "source": [
        "\"\"\"tr_va_nb_input = demand_tr.drop(\"DateTime\", axis=1).reset_index().rename({\"index\":\"idx\"},axis=1)\n",
        "tr_va_nb_input[\"group_id\"] = 0\n",
        "print(tr_va_nb_input)\n",
        "training= pf_ts_data.TimeSeriesDataSet(tr_va_nb_input, time_idx=\"idx\", group_ids=[\"group_id\"],\n",
        "                                                                     time_varying_known_reals = [\"idx\",\"X1\",\"X2\",\"X3\",\"X4\",\"date_sin\",\"date_cos\",\"day_sin\",\"day_cos\",\n",
        "                                                                                                 \"month\",\"min_cos\",\"min_sin\"],\n",
        "                                                                      time_varying_unknown_reals = [\"Y\"],\n",
        "                                        static_categoricals=[],time_varying_known_categoricals =[],\n",
        "                                       time_varying_unknown_categoricals =[],\n",
        "                                                                static_reals =[\"group_id\"],\n",
        "                                                                     target=\"Y\", min_prediction_idx=int(len(demand_tr)*0.85),\n",
        "                                                                     max_encoder_length=15,min_encoder_length=15)\n",
        "tr_nb_loader = training.to_dataloader(train=True, batch_size=b_size)\n",
        "\n",
        "validation = pf_ts_data.TimeSeriesDataSet.from_dataset(\n",
        "     training, tr_va_nb_input, predict=True, stop_randomization=True)\n",
        "va_nb_loader = validation.to_dataloader(\n",
        "     train=False, batch_size=b_size, num_workers=0)\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
